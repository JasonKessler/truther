{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still under development.  I lost the code to the original ICWSM paper, and this is my effort to reconstruct it using spaCy and Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "import os, copy\n",
    "from rdflib import ConjunctiveGraph, URIRef, Namespace, Literal\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.en.English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeriticalityElements(object):\n",
    "    def __init__(self):\n",
    "        unsorted_patterns = []\n",
    "        for file_name in self._list_veridicality_element_files():\n",
    "            for line in open(file_name).readlines():\n",
    "                ve_class_name = os.path.basename(file_name)\n",
    "                ve_tokens = line.strip().lower().split()\n",
    "                unsorted_patterns.append([-len(ve_tokens), ve_tokens, ve_class_name])\n",
    "        self._patterns = [(tokens, ve_class) \n",
    "                          for _, tokens, ve_class \n",
    "                          in sorted(unsorted_patterns)]\n",
    "    def _list_veridicality_element_files(self):\n",
    "        return glob('lexicon/*')\n",
    "    def get_patterns(self):\n",
    "        return self._patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'causals',\n",
       " 'conditionals',\n",
       " 'counter_factive_verbs',\n",
       " 'factive_verbs',\n",
       " 'negative_adjectives',\n",
       " 'negative_nouns',\n",
       " 'negative_sources',\n",
       " 'negative_verbs',\n",
       " 'neutral_verbs',\n",
       " 'positive_adjectives',\n",
       " 'positive_nouns',\n",
       " 'positive_verbs'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x for _,x in VeriticalityElements().get_patterns()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence(object):\n",
    "    def __init__(self, doc, toks, follows_facts, headof_facts, label_facts):\n",
    "        self.doc = doc\n",
    "        self.toks = toks\n",
    "        self.follows_facts = follows_facts\n",
    "        self.headof_facts = headof_facts\n",
    "        self.label_facts = label_facts\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        self._tok_index = {tok:i for i,tok in self.toks}\n",
    "        self._index_tok = {i:tok for i,tok in self.toks}\n",
    "        self._dep_index = defaultdict(set)\n",
    "        self._dauts = defaultdict(set)\n",
    "        self._heads = defaultdict(set)\n",
    "        for daut, (head, dep) in self.headof_facts:\n",
    "            self._dep_index[dep].add((daut, head))\n",
    "            self._dauts[head].add((daut, dep))\n",
    "            self._heads[daut].add((head, dep))\n",
    "        self._labels = defaultdict(set)\n",
    "        self._label_index = defaultdict(set)\n",
    "        for tok_i, label in self.label_facts:\n",
    "            self._label_index[label].add(tok_i)\n",
    "            self._labels[tok_i].add(label)\n",
    "        return self\n",
    "\n",
    "    def print_labeled(self):\n",
    "        for i,tok in self.toks:\n",
    "            #x=var()\n",
    "            print(i, tok, [label for label_i,label in self.label_facts if i == label_i])\n",
    "\n",
    "    def print_orig(self):\n",
    "        print(doc)\n",
    "    \n",
    "    def search_and_merge(self, search, label):\n",
    "        try:\n",
    "            start_idx = self._tok_index[search[0]]\n",
    "        except KeyError:\n",
    "            return self\n",
    "        if start_idx + len(search) > len(self.toks):\n",
    "            return self\n",
    "        elements_to_merge = list(range(start_idx, start_idx+len(search)))\n",
    "        # ensure that all search tokens, and not just first, match\n",
    "        if not all(self._index_tok[tok_i] == search[search_i] \n",
    "                   for search_i, tok_i in enumerate(elements_to_merge)):\n",
    "            return self\n",
    "        return self._merge_and_label_elements(elements_to_merge, label)\n",
    "    \n",
    "    def query(self,\n",
    "              deps, \n",
    "              daut_labels = set(), \n",
    "              head_labels = set(),\n",
    "              valid_dauts = set(),\n",
    "              valid_heads = set()):\n",
    "        if type(deps) == str:\n",
    "            deps = set([deps])\n",
    "        if type(daut_labels) == str:\n",
    "            daut_labels = set([daut_labels])\n",
    "        if type(head_labels) == str:\n",
    "            head_labels = set([head_labels])\n",
    "        if type(valid_dauts) == int:\n",
    "            valid_dauts = set([valid_dauts])\n",
    "        if type(valid_heads) == int:\n",
    "            valid_heads = set([valid_heads])\n",
    "        daut_labels = set(daut_labels)\n",
    "        head_labels = set(head_labels)\n",
    "        valid_dauts = set(valid_dauts)\n",
    "        valid_heads = set(valid_heads)\n",
    "        pairs = []\n",
    "        for dep in deps: \n",
    "            for daut, head in self._dep_index.get(dep,[]):\n",
    "                if daut_labels and self._labels[daut] & daut_labels == set():\n",
    "                    continue\n",
    "                if head_labels and self._labels[head] & head_labels == set():\n",
    "                    continue\n",
    "                if valid_dauts and daut not in valid_dauts:\n",
    "                    continue\n",
    "                if valid_heads and head not in valid_heads:\n",
    "                    continue\n",
    "                pairs.append((daut, head))\n",
    "        return pairs\n",
    "        \n",
    "    def _merge_and_label_elements(self, elements_to_merge, label):\n",
    "        self.label_facts.append((elements_to_merge[0], label))\n",
    "        if len(elements_to_merge) == 1:\n",
    "            return self._setup()\n",
    "        else:\n",
    "            new_toks = []\n",
    "            new_follows_facts = []\n",
    "            new_headof_facts = []\n",
    "            last_tok = None\n",
    "            headof_dict = {}\n",
    "            for tok_i, head_i in self.headof_facts:\n",
    "                headof_dict.setdefault(tok_i, []).append(head_i)\n",
    "            for i, tok in self.toks:\n",
    "                cur_tok = None\n",
    "                if i not in elements_to_merge:\n",
    "                    cur_tok = (i, tok)\n",
    "                    new_toks.append(cur_tok)\n",
    "                    for head_i, rel in headof_dict[i]:\n",
    "                        if head_i in elements_to_merge:\n",
    "                            new_headof_facts.append((i, (elements_to_merge[0], rel)))\n",
    "                        else: \n",
    "                            new_headof_facts.append((i, (head_i, rel)))\n",
    "                elif i == elements_to_merge[0]:\n",
    "                    new_string = ' '.join([tok for i, tok in self.toks \n",
    "                                           if i in elements_to_merge])\n",
    "                    new_i = elements_to_merge[0];\n",
    "                    cur_tok = (new_i, new_string)\n",
    "                    new_toks.append(cur_tok)\n",
    "                    for el_to_merge_i in elements_to_merge:\n",
    "                        for head_i, rel in headof_dict[el_to_merge_i]:\n",
    "                            if head_i not in elements_to_merge:\n",
    "                                new_headof_facts.append((new_i, (head_i, rel)))\n",
    "                if cur_tok is not None and last_tok is not None:\n",
    "                    new_follows_facts.append((last_tok[0], cur_tok[0]))\n",
    "                if cur_tok is not None:\n",
    "                    last_tok = cur_tok\n",
    "            new_sent = Sentence(doc, new_toks, new_follows_facts, new_headof_facts, self.label_facts)\n",
    "            return new_sent._setup()\n",
    "        \n",
    "def make_sentence_from_doc(doc):\n",
    "    toks = [(tok.i, tok.lower_) for tok in doc]\n",
    "    tokidx = [i for i,tok in toks]\n",
    "    follows_facts = list(zip(tokidx, (tokidx[1:] + [None])))\n",
    "    headof_facts = [(tok.i, (-1 if tok.head == tok else tok.head.i, tok.dep_)) for tok in doc]\n",
    "    return Sentence(doc, toks, follows_facts, headof_facts, [])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sam []\n",
      "1 argues []\n",
      "2 in []\n",
      "3 defense ['positive_nouns']\n",
      "4 of []\n",
      "5 the []\n",
      "6 idiot ['negative_sources']\n",
      "7 's []\n",
      "8 assertion ['positive_nouns']\n",
      "9 that []\n",
      "10 abortion is murder ['proposition']\n",
      "13 . []\n",
      "defaultdict(<class 'set'>,\n",
      "            {0: {(1, 'nsubj')},\n",
      "             1: {(-1, 'ROOT')},\n",
      "             2: {(1, 'prep')},\n",
      "             3: {(2, 'pobj')},\n",
      "             4: {(3, 'prep')},\n",
      "             5: {(6, 'det')},\n",
      "             6: {(8, 'poss')},\n",
      "             7: {(6, 'case')},\n",
      "             8: {(4, 'pobj')},\n",
      "             9: {(10, 'mark')},\n",
      "             10: {(1, 'ccomp')},\n",
      "             13: {(1, 'punct')}})\n",
      "PAIRS\n",
      "[(2, 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"Sam argues in defense of the idiot's assertion that abortion is murder.\")\n",
    "sent = make_sentence_from_doc(doc)\n",
    "sent = sent.search_and_merge('abortion is murder'.split(), 'proposition')\n",
    "veridicality_elements = VeriticalityElements()\n",
    "for pattern, label in veridicality_elements.get_patterns():\n",
    "    sent = sent.search_and_merge(pattern, label)\n",
    "sent.print_labeled();\n",
    "pprint(sent._heads)\n",
    "#print(sent._labels[1])\n",
    "#print(sent.label_facts)\n",
    "\n",
    "def heads(pairs):\n",
    "    return set(head for daut,head in pairs)\n",
    "def dauts(pairs):\n",
    "    return set(daut for daut,head in paris)\n",
    "\n",
    "#def chain(sent, query_params):\n",
    "    \n",
    "\n",
    "pairs = sent.query(['prep'], \n",
    "                   valid_heads = heads(sent.query(['ccomp', 'xcomp'], \n",
    "                                                  daut_labels='proposition')))\n",
    "print(\"PAIRS\")\n",
    "pprint(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QueryableSentence(object):\n",
    "    def _get_tok_id(self, tok_id):\n",
    "        return self.NS['tok_%s'%(tok_id)]\n",
    "    def _get_rel_id(self, child_id, head_id):\n",
    "        return self.NS['head_%s_%s'%(child_id, head_id)]\n",
    "    def __init__(self, sent: Sentence):\n",
    "        self.NS = Namespace('#')\n",
    "        self.g = ConjunctiveGraph()\n",
    "        tok_nodes = {}\n",
    "        dep_nodes = {}\n",
    "        label_nodes = {}\n",
    "        text_nodes = {}\n",
    "        for tok_id, text in sent.toks:\n",
    "            self.g.add((self._get_tok_id(tok_id),\n",
    "                        self.NS['text'],\n",
    "                        Literal(text)))            \n",
    "        for child_id, (head_id, dep) in sent.headof_facts:\n",
    "            rel_id = self._get_rel_id(child_id, head_id)\n",
    "            self.g.add((self._get_tok_id(child_id),\n",
    "                        self.NS['head'],\n",
    "                        rel_id))            \n",
    "            self.g.add((rel_id,\n",
    "                        self.NS['daut'],\n",
    "                        self._get_tok_id(head_id)))            \n",
    "            self.g.add((rel_id,\n",
    "                        self.NS['dep'],\n",
    "                        Literal(dep)))            \n",
    "        for tok_id, label_text in sent.label_facts:\n",
    "            self.g.add((self._get_tok_id(tok_id),\n",
    "                        self.NS['label'],\n",
    "                        Literal(label_text)))            \n",
    "        for left_tok_id, right_tok_id in sent.follows_facts:\n",
    "            self.g.add((self._get_tok_id(left_tok_id),\n",
    "                        self.NS['follows'],\n",
    "                        self._get_tok_id(right_tok_id)))\n",
    "    @staticmethod\n",
    "    def _get_node(nodes, id, prefix):\n",
    "        return nodes.setdefault(id, URIRef('ns:%s_%s'%(prefix,id)))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from pprint import pprint\n",
    "qs = QueryableSentence(sent) # .query('select ?s, ?r where { ?r follows ?s .}')\n",
    "#pprint(list(qs.g.objects(subject=qs.NS['tok_1'])))\n",
    "print(list(qs.g.query('''\n",
    "PREFIX ns: <#s>\n",
    "SELECT *\n",
    "WHERE { \n",
    " ?P ns:label \"proposition\" . \n",
    " ?P ns:dep ?d .\n",
    " ?X ns:head ?P .\n",
    " ?X ns:head ?_1X .\n",
    " ?_1 ns:dep ?_1d .\n",
    " ?_1 ns:head ?_2 .\n",
    " ?_2 ns:dep ?_2d .\n",
    " ?_2 ns:head ?_3 .\n",
    " ?_3 ns:dep ?_3d .\n",
    " ?_3 ns:head ?_4 .\n",
    " ?_4 ns:dep ?_4d .\n",
    " ?_4 ns:head ?VE .\n",
    " ?VE ns:dep ?_VEd .\n",
    " ?VE ns:label ?VE_label . \n",
    "FILTER \n",
    "((?d = \"ccomp\" || ?d = \"xcomp\") \n",
    " && (?_1d = \"prep\") \n",
    " && (?_2d = \"pobj\") \n",
    " && (?_3d = \"prep\") \n",
    " && (?_4d = \"pobj\") \n",
    " && (?_VEd = \"poss\") \n",
    " && (?VE_label = \"negative_sources\")\n",
    ") .\n",
    "} \n",
    "''')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_clause(tok1, tok2, rel=[], tok1_label = [], tok2_label = [], tok1_text = []):\n",
    "    fmt_dict = {'tok1': tok1, 'tok2': tok2}\n",
    "    filter_clauses = []\n",
    "    where_clauses = ['?%(tok1)s ns:head ?rel_%(tok1)s_%(tok2)s' % fmt_dict,\n",
    "                     '?rel_%(tok1)s_%(tok2)s ns:daut ?%(tok2)s' % fmt_dict]\n",
    "    def perform_filter(relation, node_id, possible_values=[]):\n",
    "        if possible_values != []:\n",
    "            where_clauses.append('?%(node_id)s ns:%(rel)s ?%(rel)s_%(node_id)s' \n",
    "                                 % {'node_id':node_id, 'rel':relation})\n",
    "            conjuncts = []\n",
    "            for value in possible_values:\n",
    "                conjuncts.append('?%(rel)s_%(node_id)s = \"%(value)s\"' \n",
    "                                 % {'node_id':node_id, 'rel':relation, 'value': value})\n",
    "            filter_clauses.append('(%s)' % ' || '.join(conjuncts))\n",
    "    perform_filter('dep', 'rel_%(tok1)s_%(tok2)s' % fmt_dict, rel)    \n",
    "    perform_filter('label', '%(tok1)s' % fmt_dict, tok1_label)    \n",
    "    perform_filter('label', '%(tok2)s' % fmt_dict, tok2_label)    \n",
    "    if tok1_label != []:\n",
    "        pass\n",
    "    return ' . '.join(where_clauses) + ' .', ' && '.join(filter_clauses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert query_clause('A', 'B') == ('?A ns:head ?rel_A_B . ?rel_A_B ns:daut ?B .', '') \n",
    "assert query_clause('C', 'D') == ('?C ns:head ?rel_C_D . ?rel_C_D ns:daut ?D .', '') \n",
    "assert query_clause('C', 'D', rel=['xcomp'])[1] == '(?dep_rel_C_D = \"xcomp\")'\n",
    "assert query_clause('C', 'D', rel=['xcomp'])[0] \\\n",
    "    == '?C ns:head ?rel_C_D . ?rel_C_D ns:daut ?D . ?rel_C_D ns:dep ?dep_rel_C_D .'\n",
    "assert query_clause('C', 'D', rel=['xcomp', 'ccomp'])[1] == '(?dep_rel_C_D = \"xcomp\" || ?dep_rel_C_D = \"ccomp\")'\n",
    "assert query_clause('C', 'D', tok1_label=['negative_sources'])[1] == '(?label_C = \"negative_sources\")'\n",
    "assert query_clause('C', 'D', tok1_label=['dfs', 'blah'])[1] == '(?label_C = \"dfs\" || ?label_C = \"blah\")'\n",
    "assert query_clause('C', 'D', tok1_label=['dfs', 'blah'])[0] \\\n",
    "    == '?C ns:head ?rel_C_D . ?rel_C_D ns:daut ?D . ?C ns:label ?label_C .'\n",
    "assert query_clause('C', 'D', tok2_label=['dfs', 'blah'])[1] == '(?label_D = \"dfs\" || ?label_D = \"blah\")'\n",
    "assert query_clause('C', 'D', tok2_label=['dfs', 'blah'])[0] \\\n",
    "    == '?C ns:head ?rel_C_D . ?rel_C_D ns:daut ?D . ?D ns:label ?label_D .'\n",
    "assert query_clause('C', 'D', tok1_label=['A'], tok2_label=['B'])[0] \\\n",
    "    == '?C ns:head ?rel_C_D . ?rel_C_D ns:daut ?D . ?C ns:label ?label_C . ?D ns:label ?label_D .'\n",
    "assert query_clause('C', 'D', tok1_label=['A'], tok2_label=['B'])[1] \\\n",
    "    == '(?label_C = \"A\") && (?label_D = \"B\")'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_builder(variables, clauses):\n",
    "    where_clauses = []\n",
    "    filter_clauses = []\n",
    "    for where_clause, filter_clause in clauses:\n",
    "        where_clauses.append(where_clause)\n",
    "        if filter_clause != '':\n",
    "            filter_clauses.append(filter_clause)\n",
    "    filter_portion = ''\n",
    "    if filter_clauses != []:\n",
    "        filter_portion = 'FILTER ( %s )' % ' && '.join(filter_clauses)\n",
    "    query = '''\n",
    "PREFIX ns: <#>\n",
    "SELECT %(variables)s\n",
    "WHERE { \n",
    " %(where_clauses)s\n",
    " %(filter_portion)s\n",
    "}''' % {'variables': ' '.join('?'+v for v in variables), \n",
    "        'where_clauses': ' '.join(where_clauses),\n",
    "        'filter_portion': filter_portion}\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pprint(list(zip(qs.g.subjects(), qs.g.predicates(), qs.g.objects())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc = nlp('Sam argues in defense of the idiot’s assertion that abortion is murder.')\n",
    "sent = make_sentence_from_doc(doc)\n",
    "sent = sent.search_and_merge('abortion is murder'.split(), 'proposition')\n",
    "veridicality_elements = VeriticalityElements()\n",
    "for pattern, label in veridicality_elements.get_patterns():\n",
    "    sent = sent.search_and_merge(pattern, label)\n",
    "sent.print_labeled()\n",
    "pprint(sent.headof_facts)\n",
    "\n",
    "query = query_builder(['P', 'VE'], \n",
    "                      [query_clause('X', 'P', rel=['ccomp', 'xcomp']),\n",
    "                       query_clause('X', 'A', rel=['prep']),\n",
    "                       query_clause('A', 'B', rel=['pobj']),\n",
    "                       query_clause('B', 'C', rel=['prep']),\n",
    "                       query_clause('C', 'D', rel=['pobj']),\n",
    "                       query_clause('D', 'VE', rel=['poss'])])\n",
    "qs = QueryableSentence(sent)\n",
    "print(list(qs.g.query(query)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mq = '''\n",
    "PREFIX ns: <#s>\n",
    "SELECT ?A ?B\n",
    "WHERE { \n",
    " ?A ns:head ?rel_A_B .\n",
    "?rel_A_B ns:daut ?B .\n",
    " \n",
    "}\n",
    "'''\n",
    "print(list(qs.g.query(mq)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QueryableSentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b373274ed95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryableSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# .query('select ?s, ?r where { ?r follows ?s .}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m print(list(qs.g.query('''\n\u001b[1;32m      5\u001b[0m \u001b[0mPREFIX\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;31m#>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QueryableSentence' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "qs = QueryableSentence(sent) # .query('select ?s, ?r where { ?r follows ?s .}')\n",
    "print(list(qs.g.query('''\n",
    "PREFIX ns: <#>\n",
    "SELECT ?P\n",
    "WHERE { \n",
    " ?P ns:label \"proposition\" . \n",
    " ?P ns:head ?rel_tok1_2 .\n",
    " ?rel_tok1_2 ns:daut ?_tok2 . \n",
    " ?rel_tok1_2 ns:dep ?dep1_2 .\n",
    "\n",
    " ?rel_tok2_3 ns:daut ?_tok2 . \n",
    " ?rel_tok2_3 ns:dep ?dep2_3 .\n",
    " ?tok_3 ns:head ?rel_tok2_3 .\n",
    " \n",
    " ?rel_tok3_4 ns:daut ?_tok3 . \n",
    " ?rel_tok3_4 ns:dep ?dep2_3 .\n",
    " ?tok_3 ns:head ?rel_tok3_4 .\n",
    "\n",
    "\n",
    "\n",
    "FILTER  (\n",
    "  (?dep1_2 = \"ccomp\" || ?dep1_2 = \"xcomp\")\n",
    "  && ?dep2_3 = \"prep\"\n",
    " ) \n",
    "}\n",
    "''')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-393399e2a28d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mveridicality_elements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_and_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_labeled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryableSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# .query('select ?s, ?r where { ?r follows ?s .}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0b43f9dafe9f>\u001b[0m in \u001b[0;36mprint_labeled\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_labeled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_facts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'var' is not defined"
     ]
    }
   ],
   "source": [
    "#from pprint import pprint\n",
    "doc = nlp('Sam argues in defense of the idiot’s assertion that abortion is murder.')\n",
    "sent = make_sentence_from_doc(doc)\n",
    "sent = sent.search_and_merge('abortion is murder'.split(), 'proposition')\n",
    "veridicality_elements = VeriticalityElements()\n",
    "for pattern, label in veridicality_elements.get_patterns():\n",
    "    sent = sent.search_and_merge(pattern, label)\n",
    "sent.print_labeled()\n",
    "\n",
    "qs = QueryableSentence(sent) # .query('select ?s, ?r where { ?r follows ?s .}')\n",
    "#pprint(list(qs.g.objects(subject=qs.NS['tok_1'])))\n",
    "print(list(qs.g.query('''\n",
    "PREFIX ns: <#s>\n",
    "SELECT *\n",
    "WHERE { \n",
    " ?P ns:label \"proposition\" . \n",
    " ?P ns:dep ?d .\n",
    " ?X ns:head ?P .\n",
    " ?X ns:head ?_1X .\n",
    " ?_1 ns:dep ?_1d .\n",
    " ?_1 ns:head ?_2 .\n",
    " ?_2 ns:dep ?_2d .\n",
    " ?_2 ns:head ?_3 .\n",
    " ?_3 ns:dep ?_3d .\n",
    " ?_3 ns:head ?_4 .\n",
    " ?_4 ns:dep ?_4d .\n",
    " ?_4 ns:head ?VE .\n",
    " ?VE ns:dep ?_VEd .\n",
    " ?VE ns:label ?VE_label . \n",
    " FILTER \n",
    "  ((?d = \"ccomp\" || ?d = \"xcomp\") \n",
    "   && (?_1d = \"prep\") \n",
    "   && (?_2d = \"pobj\") \n",
    "   && (?_3d = \"prep\") \n",
    "   && (?_4d = \"pobj\") \n",
    "   && (?_VEd = \"poss\") \n",
    "   && (?VE_label = \"negative_sources\")\n",
    "  ) .\n",
    "} \n",
    "''')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
